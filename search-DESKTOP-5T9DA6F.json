[
  {
    "objectID": "slide-embed.html",
    "href": "slide-embed.html",
    "title": "Embed Slides",
    "section": "",
    "text": "On this page, we show how we can embed a RevealJS Presentation inside of a Quarto Website."
  },
  {
    "objectID": "slide-embed.html#presentation",
    "href": "slide-embed.html#presentation",
    "title": "Embed Slides",
    "section": "Presentation",
    "text": "Presentation\n\n\n\n\n\n\nImportant\n\n\n\nFor quarto-webr to work within RevealJS, you must use a pre-release version of Quarto that is 1.4.502 or greater that contains an updated copy of pandoc. For more details, please see Issue #14."
  },
  {
    "objectID": "slide-embed.html#embed-code",
    "href": "slide-embed.html#embed-code",
    "title": "Embed Slides",
    "section": "Embed Code",
    "text": "Embed Code\nPlace the following code inside of the Quarto Document:\n&lt;style&gt;\n.slide-deck {\n    border: 3px solid #dee2e6;\n    width: 100%;\n    height: 475px;\n}\n&lt;/style&gt;\n\n&lt;div&gt;\n```{=html}\n&lt;iframe class=\"slide-deck\" src=\"path/to/presentation/\"&gt;&lt;/iframe&gt;\n```\n&lt;/div&gt;"
  },
  {
    "objectID": "Lab3.html",
    "href": "Lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Lab 3 covers\n\nMissing data\n\n\nMissing mechanism\nMissing completely at random (MCAR), Missing at random (MAR), and Missing not at random (MNAR)\nThese are assumptions that cannot be directly verified using the data.\n\nExample writing of your method section\n\n\nMissing data were handled using multiple imputation by chained equations, assuming data were missing at random. The imputation model included the covariates used in the outcome analysis (XXX, YYY, and ZZZ), the treatment variable, the outcome variable, and ABC as an auxiliary variable. XXX regression models were fit to each imputed dataset, and the results from 100 imputations were combined using Rubin’s rules. … As a sensitivity analysis, we performed a complete case analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nMy favoriate workflow\n\nPackage-dependent practice\n\nThe standard mice workflow often uses with() and pool().\n# 1. IMPUTE: Create 10 imputed datasets\nimp_multi &lt;- mice(support, m = 10, printFlag = FALSE, seed = 123)\n\n# It's good practice to save the imputation object\n# write_rds(imp_multi, 'imputed_data.rds')\n\n# 2. ANALYZE: Run the model on each imputed dataset\n# The 'with()' function does this automatically\nanalysis_results &lt;- with(imp_multi,\nglm(death ~ dzclass + age + sex + edu + income + race + diabetes + wblc,\nfamily = binomial))\n\n# 3. POOL: Combine the results using Rubin's Rules\npooled_results &lt;- pool(analysis_results)\n\nMy favorite workflow\n\nHowever, you may encounter situations where existing packages do not support your study design (e.g., performing propensity score analysis with multiple imputation).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nMultiple imputation\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nStack\n\nConvert your multiply imputed datasets into a single dataframe where each imputed dataset is stacked on top of each other.\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nNest\n\n\ngroup_by(.imp) -&gt; Treat each imputed dataset separately\nnest() -&gt; Creates a list-column where each row contains one dataset (data column)\nmutate(fit = map(...)) -&gt; Iterate over the list of datasets, run the model on each one, and store the model object in a new list-column called fit.\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nRubin’s rule\n\nNow we can pool the results using the mitools package.\nsummary(mitools::MIcombine(imp_analysis$fit))\nYou can use coefs and vcovs, instead of fit.\n\n\nMore advanced coding\nYou can speed up your work by using data.table and speedglm.\nimp_long_dt &lt;- complete(imp_multi, action = \"long\")\nsetDT(imp_long_dt)\n\nlibrary(speedglm)\n\nimp_analysis &lt;- imp_long_dt[, {\n  # For each group, fit the logistic regression model\n  model &lt;- speedglm(death ~ dzclass + age + sex + edu + income + race + diabetes + wblc,\n                    family = binomial(), data = .SD)\n  # Return a list containing the coefficients and vcov matrix for this group\n  # These are wrapped in an outer list() to create list-columns.\n  list(coefs = list(coef(model)), vcovs = list(vcov(model)))\n}, by = .imp]\n\npooled_results &lt;- mitools::MIcombine(imp_analysis$coefs, imp_analysis$vcovs)\nsummary(pooled_results)",
    "crumbs": [
      "Home",
      "Content",
      "Lab 3"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome!\nThis website is a collection of lab materials for the EPID8313 course (2025). The labs are designed to help you practice and apply the concepts covered in the course.\nDuring each lab session (typically held before the journal club), I will address your specific questions using the resources provided here. To allow sufficient preparation time, please submit your questions at least one day in advance. If no questions are submitted, I will review the core concepts of the lab material for about 10–15 minutes before transitioning to the journal club.\nFor the journal club, you should prepare several slides summarizing the assigned articles. Each presentation will typically take around 15 minutes per article.\n\n\nInstroctor\n\nAki Shiroshita, MD, MPH\n4th-year Epidemiology PhD student\nOriginally from Japan\nFulbright grantee\nResearch interest: Environmental epidemiology, pharmacoepidemiology\nDissertation: Traffic-related environmental exposures during early life and adverse health outcomes",
    "crumbs": [
      "Home",
      "Content",
      "Overview"
    ]
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Comments on the assignment 1\n\nI prefer to provide a simple framework and/or figures alongside the text.\n\nQ1: the PECO format\nQ3: subject selection flow\nQ6: DAG\n\nInformation bias\n\nMeasurement error: mismeasurment of continuous variables (sometimes, random error, not systemic error, is called “measurement error”)\nMisclassification: inaccuracy of categorical variables (sometimes, this is called “information bias”)\n\n\nDAG may help understand the bias\n\nSelection bias\n\n\n\n\n\n\n\n\n\n\n\nMisclassification (Differentiality vs. Dependence)\n\n\nDifferentiality\nNondifferential: Sensitivity and specificity are independent of another key variable\nvs. \nDifferential: Sensitivity and specificity are not independent of another key variable\n\n\nDependence\nIndependent: Misclassification of one variable does not change the probability that subjects will be misclassified on a second variable\nvs. \nDependent: The probability of being doubly misclassified is not equal to the product of the probabilities of being singly misclassified with respect to exposure and outcome. (e.g., sensitivity of exposure misclassification is correlated with sensitivity of outcome misclassification)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPresentation\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Home",
      "Content",
      "Assignment 1"
    ]
  },
  {
    "objectID": "Lab2.html",
    "href": "Lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Lab 2 covers\n\ndata wrangling\nregression models.\n\n\nData wrangling\n\nbase R\ntidyverse: clean and readable\ndata.table: Optimized for speed and memory efficiency\n\n\n\n\n\n\n\nNote\n\n\n\nAs I use very large-scale environmental data, I am switching from tidyverse to data.table.\n\n\n\n\ntidyverse\nA philosophy created by Hadley Wickham\n\ndplyr → data manipulation (filter, mutate, group, summarize)\nggplot2 → data visualization\ntidyr → data tidying (reshaping, pivoting, separating)\nreadr → data import (CSV, TSV, etc.)\ntibble → modern data frames (better printing, easier handling)\npurrr → functional programming with lists (map, reduce)\nstringr → string handling\nforcats → categorical (factor) variables handling\n\n\n\n\n\n\n\nNote\n\n\n\nThe pipe function (takes the result of the expression on its left and “pipes” it into the function on its right) %&gt;% makes the workflow read like a sequence of steps.\n\n\nDemonstrations\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nMy tips and tricks\n\nFor moderate-sized datasets, I recommend using tidyverse for its readability.\nMemorize the basic commands for descriptive analyses.\nCheck the unique values and summaries of each column (e.g., unique(), summary())\nCheck a lot of contingency tables (e.g., table())\nVisualize the variables as needed (e.g., ggplot())\n\n\n\ndata.table (advanced)\n \n\nDT[i, j, by]\n\n\nWhere DT is a data.table object, the i argument is used for filtering and joining, the j argument is used for summarizing and transforming, and the by argument defines the groups to which to apply these operations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nConvert objects to data.table:\n\nsetDT() vs. as.data.table()\nsetDT(): Converts in place\nas.data.table(): Creates a new data.table object\n\nFilter rows\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nChange data type\n\nModify-in-place\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nMake new columns\n\nModify-in-place\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSummarize by group\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPiping\n\nIt takes the result of the expression on its left and “pipes” it into the function on its right.\nAfter R 4.3.0, the native pipe supports a _ placeholder to the right-hand side of the pipe.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPoisson regression\n\nConventional Poisson regression\n\nFor count data\n\\(log(E[Y_i|X_i])=\\beta_0+\\beta_1X_i+log(T_i)\\)\nEffect measure: rate ratio\n\nModified Poisson regression\n\nFor binary data\n\\(log(E[Y_i|X_i])=\\beta_0+\\beta_1X_i\\)\nEffect measure: risk ratio\n\n\n\n\n\n\nNote\n\n\n\nRobust standard errors are recommended in both the conventional Poisson and the modified Poisson regression models.\n\n\n\n\nRegression models (advanced)\nspeedglm package: more speedy than glm\nIt directly solves the normal equations of the weighted least squares problem at each iteration (rather than relies on a QR decomposition).\nWhy does this important?\n\\(\\rightarrow\\) In G-methods, you often create many derived variables and fit many regression models across “copied” datasets.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Home",
      "Content",
      "Lab 2"
    ]
  },
  {
    "objectID": "Lab4.html",
    "href": "Lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "Lab 4 covers\n\nPower and sample size calculation\n\nIn many cases, we want to perform power calculations for multivariable regression models.\n\n\n\n\n\n\nNote\n\n\n\nPower = probability of rejecting the null hypothesis if a specific alternative is true\n\n\n\nBrief overview of software\n\nR packages: pwr, pwrss, etc.\nSTATA\nSAS Proc Power\nPS: Power and Sample Size Calculation (Vanderbilt University, free): https://cqsclinical.app.vumc.org/ps/\nPASS (NCSS, commercial, most flexible): https://www.ncss.com/software/pass/\nEPI Info (CDC, free): https://www.cdc.gov/epiinfo/index.html\nFor conducting complex statistical analyses (e.g., propensity score analysis), I recommend using Monte Carlo simulations.\n\n\n\nR pwrss package\nTutorial: https://rpubs.com/metinbulus/pwrss\nPower and sample size calculation based on the Wald test.\nScenario: You are planning a study where the outcome is binary and will be analyzed with logistic regression.\nWhat you need is\n\np0: probability of the outcome when the predictor is at reference level (e.g., control group).\np1: probability of the outcome when the predictor is at the exposure level (e.g., treatment group).\nr2.other.x: A squared multiple correlation between an exposure and other covariates\n\nHigher values indicate more correlation, which will increase the required sample size.\nYou can calculate r2.other.x by fitting a linear regression model where your main predictor of interest is treated as the outcome and all other covariates are the predictors.\nfit &lt;- lm(exposure ~ cov1+cov2, data = data)\n# Extract the R-squared value\nsummary_model &lt;- summary(fit)\nr2_estimate &lt;- summary_model$r.squared\n\nn: total sample size\nalpha = 0.05: significance level\n\n\npwrss::pwrss.z.logreg(\n  p0 = 0.11,\n  p1 = 0.10,\n  r2.other.x = 0.10,\n  n = 8000,\n  alpha = 0.05\n)\n\n Logistic Regression Coefficient \n (Large Sample Approx. Wald's z Test) \n H0: beta1 = 0 \n HA: beta1 != 0 \n Distribution of X = 'normal' \n Method = DEMIDENKO(VC) \n ------------------------------ \n  Statistical power = 0.807 \n  n = 8000 \n ------------------------------ \n Alternative = \"not equal\" \n Non-centrality parameter = -2.827 \n Type I error rate = 0.05 \n Type II error rate = 0.193 \n\n\n\nExample writing of your method section\n\n\nFor our primary analysis, a sample size of 8,000 provides approximately 81% power to detect an odds ratio of 0.90 for the main exposure of interest, assuming a baseline outcome prevalence of 11%. This calculation was performed using a two-sided z-test at a significance level of 0.05 and accounts for potential confounding by assuming that other covariates explain 10% of the variance in the exposure (i.e., a squared multiple correlation of 0.1 between the primary exposure and other covariates)\n\n\n\n\n\n\n\nNote\n\n\n\n\nOdds for the reference group: \\(0.11/(1−0.11)=0.1236\\)\nOdds for the comparison group: \\(0.10/(1−0.10)=0.1111\\)\nOdds Ratio \\(= 0.1111/0.1236 \\approx 0.90\\)",
    "crumbs": [
      "Home",
      "Content",
      "Lab 4"
    ]
  }
]